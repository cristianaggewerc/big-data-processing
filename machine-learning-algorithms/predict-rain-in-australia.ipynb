{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 | FIT 5202 - Data processing for Big Data\n",
    "\n",
    "\n",
    "Student: Cristiana Garcia Gewerc\n",
    "\n",
    "ID: 3008887\n",
    "\n",
    "## A. Creating Spark Session and Loading the Data\n",
    "\n",
    "### Step 01: Import Spark Session and initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". \n",
    "# If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    # local[4]: run Spark locally with 4 working processors.\n",
    "    sc = SparkContext(master=\"local[4]\", appName=\"Assignment 2 FIT 5202\")\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 02: Load the dataset and print the schema and total number of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 142193 lines\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "weather_df = spark.read.csv('weatherAUS.csv', header='true')\n",
    "\n",
    "# display total number of lines\n",
    "print(\"The dataset has %d lines\" %weather_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema to analyse which are all the columns and what are their types\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Data Cleaning and Processing\n",
    "Data cleaning and processing is an important aspect for any machine learning task. We have to carefully look into the data and based on the types, quality of the data, we have to plan our cleaning procedures.\n",
    "### Step 03: Delete columns from the dataset\n",
    "During the data cleaning and processing phase, we delete unnecessary data from the dataset to improve the efficiency and accuracy of our model. You have to think which columns are not contributing to the rain prediction. To keep things simple, you are required to delete the following columns due to data quality and accuracy. <br>\n",
    "● Date <br>\n",
    "● Location <br>\n",
    "● Evaporation <br>\n",
    "● Sunshine <br>\n",
    "● Cloud9am <br>\n",
    "● Cloud3pm <br>\n",
    "● Temp9am <br>\n",
    "● Temp3pm <br>\n",
    "However, if you want to keep any of these columns, you can keep them if you process them in an intelligent way that improve the accuracy, that is fine, ​ however not mandatory​ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop above listed columns\n",
    "weather_df = weather_df.drop(*['Date', 'Location', 'Evaporation', 'Sunshine', \n",
    "                               'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 04: Print the number of missing data in each column.\n",
    "We already have an initial idea about the data structure from the schema. Even in\n",
    "plain eyes, we can observe that there are lots of NA (null) values in the given dataset.\n",
    "Your job in this step is to print the number of NA(null) values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|MinTemp|\n",
      "+-------+\n",
      "|    637|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|MaxTemp|\n",
      "+-------+\n",
      "|    322|\n",
      "+-------+\n",
      "\n",
      "+--------+\n",
      "|Rainfall|\n",
      "+--------+\n",
      "|    1406|\n",
      "+--------+\n",
      "\n",
      "+-----------+\n",
      "|WindGustDir|\n",
      "+-----------+\n",
      "|       9330|\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|WindGustSpeed|\n",
      "+-------------+\n",
      "|         9270|\n",
      "+-------------+\n",
      "\n",
      "+----------+\n",
      "|WindDir9am|\n",
      "+----------+\n",
      "|     10013|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|WindDir3pm|\n",
      "+----------+\n",
      "|      3778|\n",
      "+----------+\n",
      "\n",
      "+------------+\n",
      "|WindSpeed9am|\n",
      "+------------+\n",
      "|        1348|\n",
      "+------------+\n",
      "\n",
      "+------------+\n",
      "|WindSpeed3pm|\n",
      "+------------+\n",
      "|        2630|\n",
      "+------------+\n",
      "\n",
      "+-----------+\n",
      "|Humidity9am|\n",
      "+-----------+\n",
      "|       1774|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|Humidity3pm|\n",
      "+-----------+\n",
      "|       3610|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|Pressure9am|\n",
      "+-----------+\n",
      "|      14014|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|Pressure3pm|\n",
      "+-----------+\n",
      "|      13981|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|RainToday|\n",
      "+---------+\n",
      "|     1406|\n",
      "+---------+\n",
      "\n",
      "+------------+\n",
      "|RainTomorrow|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, isnan\n",
    "# define function to count \"NA\"/NaN/NULL:\n",
    "def count_na(column):\n",
    "    weather_df.select(\n",
    "    count(when(\n",
    "        (weather_df[column]=='NA')|(weather_df[column].isNull()| (isnan(weather_df[column]))),\n",
    "        weather_df[column]))\n",
    "    .alias(column)).show()\n",
    "# apply function to each column of our spark dataframe:\n",
    "for column in weather_df.columns:\n",
    "    count_na(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 05: Fill the missing data with average value and maximum occurrence value.\n",
    "In this step you have to fill in all the missing data with average value (for numeric column) or maximum frequency value (for non-numeric column).\n",
    "\n",
    "1) Firstly, identify the columns which have numeric values (e.g., MinTemp, MaxTemp), calculate the average and fill the null value with the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MinTemp='13.4', MaxTemp='22.9', Rainfall='0.6', WindGustDir='W', WindGustSpeed='44', WindDir9am='W', WindDir3pm='WNW', WindSpeed9am='20', WindSpeed3pm='24', Humidity9am='71', Humidity3pm='22', Pressure9am='1007.7', Pressure3pm='1007.1', RainToday='No', RainTomorrow='No')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue><b> Now that we can see which are the numeric columns ('MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm') we will calculate their averages:</font></b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('summary', 'mean')\n",
      "('MinTemp', '12.186399728729098')\n",
      "('MaxTemp', '23.226784191272444')\n",
      "('Rainfall', '2.3499740743111954')\n",
      "('WindGustSpeed', '39.98429165757619')\n",
      "('WindSpeed9am', '14.001988000994')\n",
      "('WindSpeed3pm', '18.63757586179718')\n",
      "('Humidity9am', '68.8438103105705')\n",
      "('Humidity3pm', '51.482606091656265')\n",
      "('Pressure9am', '1017.6537584159781')\n",
      "('Pressure3pm', '1015.258203537907')\n"
     ]
    }
   ],
   "source": [
    "num_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', \n",
    "            'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n",
    "            'Humidity3pm','Pressure9am', 'Pressure3pm']\n",
    "# collecting the means of the \"num_cols\"\n",
    "num_mean = weather_df.select(num_cols).describe().collect()[1]\n",
    "# printing the means with help of dictinary format\n",
    "for item in num_mean.asDict().items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Answer:</font></b> <font color=blue><b>\n",
    "The columns which have numerical values and their respective averages are:<br>\n",
    "    . MinTemp: 12.19 <br>\n",
    "    . MaxTemp: 23.23<br>\n",
    "    . Rainfall: 2.34<br>\n",
    "    . WindGustSpeed: 39.98<br>\n",
    "    . WindSpeed9am: 14.00<br>\n",
    "    . WindSpeed3pm: 18.64<br>\n",
    "    . Humidity9am: 68.84<br>\n",
    "    . Humidity3pm: 51.48<br>\n",
    "    . Pressure9am: 1017.65<br>\n",
    "    . Pressure3pm: 1015.26<br>\n",
    "Now, we are going to replace their \"NA\" entries with the average values.\n",
    "</font></b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for col in num_cols: # for each numeric column, impute the corresponding mean in case its \"NA\", NULL or NaN\n",
    "    weather_df = weather_df.withColumn(col,\n",
    "              when(\n",
    "                  (weather_df[col]=='NA')|(weather_df[col].isNull()| (isnan(weather_df[col])))\n",
    "                  , num_mean[i]).otherwise(weather_df[col]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Secondly, identify the columns with non-numeric values (e.g., WindGustDir, WindDir9am)\n",
    "and find the most frequent item (e.g., wind direction). \n",
    "\n",
    "<font color=blue><b>All other columns present non-numeric values, i.e. 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow'. Let's find their most fequent item:</font></b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(WindGustDir='W', count=9780), Row(WindDir9am='N', count=11393), Row(WindDir3pm='SE', count=10663), Row(RainToday='No', count=109332), Row(RainTomorrow='No', count=110316)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "nom_num_cols = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
    "# collecting the most frequent of values of the \"non_num_cols\"\n",
    "nom_num_modes =[]\n",
    "for col in nom_num_cols:\n",
    "    nom_num_modes.append(weather_df.groupBy(col).count().sort(desc(\"count\")).collect()[0])\n",
    "# printing the modes\n",
    "print(nom_num_modes)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Answer:</font></b> <font color=blue><b>\n",
    "The columns which have non-numerical values and their respective averages are:<br>\n",
    "    . WindGustDir: 'W'<br>\n",
    "    . WindDir9am: 'N'<br>\n",
    "    . WindDir3pm: 'SE'<br>\n",
    "    . RainToday: 'No'<br>\n",
    "    . RainTomorrow: 'No'<br>\n",
    "Now, we are going to replace their \"NA\" entries with their max occurence values.\n",
    "</font></b> \n",
    "\n",
    "\n",
    "3) Now fill the null values with that item for that particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for col in nom_num_cols:\n",
    "    # nom_num_modes is a list of one element per non numeric col. FOr each one of them, the first [0] element is\n",
    "    # the mode of the corresponding column\n",
    "    weather_df = weather_df.withColumn(col,\n",
    "              when((weather_df[col]=='NA')|(weather_df[col].isNull()| (isnan(weather_df[col]))),\n",
    "                   nom_num_modes[i][0]).otherwise(weather_df[col]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 06: Data transformation\n",
    "In this step, you need to transform the data so that it will be useful to process by the machine learning algorithm. Before transforming your non-numerical data, do the type casting (to double) of the numerical value columns as they are defined as “String” (see, the schema of the dataset). For the non-numerical value column (i.e., WindGustDir, WindDir9am, WindDir3pm, RainTomorrow) use the StringIndexer method to convert them into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- WindGustDir_index: double (nullable = false)\n",
      " |-- WindDir9am_index: double (nullable = false)\n",
      " |-- WindDir3pm_index: double (nullable = false)\n",
      " |-- RainToday_index: double (nullable = false)\n",
      " |-- RainTomorrow_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# numerical data\n",
    "for col in num_cols:\n",
    "    weather_df = weather_df.withColumn(col, weather_df[col].cast('double'))\n",
    "\n",
    "# non-numerical\n",
    "cat_indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(weather_df) for col in nom_num_cols]\n",
    "ml_pipeline = Pipeline(stages=cat_indexers)\n",
    "weather_df = ml_pipeline.fit(weather_df).transform(weather_df).drop(*nom_num_cols)\n",
    "\n",
    "# checking new schema\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 07: Create the feature vector and divide the dataset\n",
    "In this step, you have to create the feature vector from the given columns. When you create you feature vector, remember to exclude the column that you will be using for testing the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(MinTemp=13.4, MaxTemp=22.9, Rainfall=0.6, WindGustSpeed=44.0, WindSpeed9am=20.0, WindSpeed3pm=24.0, Humidity9am=71.0, Humidity3pm=22.0, Pressure9am=1007.7, Pressure3pm=1007.1, WindGustDir_index=0.0, WindDir9am_index=6.0, WindDir3pm_index=7.0, RainToday_index=0.0, RainTomorrow_index=0.0, features=DenseVector([13.4, 22.9, 0.6, 44.0, 20.0, 24.0, 71.0, 22.0, 1007.7, 1007.1, 0.0, 6.0, 7.0, 0.0]))]\n",
      "\n",
      "Only label and features: \n",
      " [Row(RainTomorrow_index=0.0, features=DenseVector([13.4, 22.9, 0.6, 44.0, 20.0, 24.0, 71.0, 22.0, 1007.7, 1007.1, 0.0, 6.0, 7.0, 0.0]))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#feature_cols = list(set(weather_df.columns) - {'RainTomorrow_index'})\n",
    "\n",
    "feature_cols = ['MinTemp','MaxTemp','Rainfall', 'WindGustSpeed', 'WindSpeed9am','WindSpeed3pm', 'Humidity9am',\n",
    "                'Humidity3pm', 'Pressure9am', 'Pressure3pm',\n",
    "                'WindGustDir_index','WindDir9am_index', 'WindDir3pm_index', 'RainToday_index']\n",
    "\n",
    "\n",
    "\n",
    "# creating the feature vector\n",
    "vector_assembler = VectorAssembler(inputCols= feature_cols, outputCol=\"features\")\n",
    "\n",
    "df_temp = vector_assembler.transform(weather_df)\n",
    "print(df_temp.take(1))\n",
    "\n",
    "weather_df = df_temp['RainTomorrow_index', 'features']\n",
    "print(\"\\nOnly label and features: \\n\", weather_df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation of your feature vector, you have to split your dataset into two (e.g., training and testing). In this assignment, you have to spit the dataset randomly and between 70 percent and 30 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset between training and test dataset (setting seed to have reproducible results):\n",
    "(training_data, test_data) = weather_df.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Apply Machine Learning Algorithms\n",
    "### Step 08: Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph.\n",
    "You have to use DecisionTreeClassifier(), RandomForestClassifier(), and LogisticRegression(), GBTClassifier() methods in spark to calculate the probability of the rain fall tomorrow based on the other related data points (e.g., temperature, wind, humidity).\n",
    "\n",
    "1. Run and calculate the accuracy of decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|RainTomorrow_index|\n",
      "+----------+------------------+\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.165842 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create, fit and predict the model\n",
    "dt = DecisionTreeClassifier(labelCol=\"RainTomorrow_index\", featuresCol=\"features\", seed = 1234)\n",
    "dt_model = dt.fit(training_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "dt_predictions.select(\"prediction\", \"RainTomorrow_index\").show(5)\n",
    "\n",
    "## accuracy evaluator for all models ##\n",
    "evaluator = MulticlassClassificationEvaluator(\\\n",
    "labelCol=\"RainTomorrow_index\", predictionCol=\"prediction\",\\\n",
    "metricName=\"accuracy\")\n",
    "\n",
    "# testing decision tree accuracy\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run and calculate the accuracy of random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|RainTomorrow_index|\n",
      "+----------+------------------+\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.165372\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# create, fit and predict the model\n",
    "rf = RandomForestClassifier(labelCol=\"RainTomorrow_index\", featuresCol=\"features\", numTrees=10, seed=1234)\n",
    "rf_model = rf.fit(training_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "rf_predictions.select(\"prediction\", \"RainTomorrow_index\").show(5)\n",
    "\n",
    "# testing random forest accuracy\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run and calculate the accuracy of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|RainTomorrow_index|\n",
      "+----------+------------------+\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.18534\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create, fit and predict the model\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'RainTomorrow_index', maxIter=10)\n",
    "lr_model = lr.fit(training_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_predictions.select(\"prediction\", \"RainTomorrow_index\").show(5)\n",
    "\n",
    "# testing logistic regression accuracy\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run and calculate the accuracy of gradient boosted trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|RainTomorrow_index|\n",
      "+----------+------------------+\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "|       0.0|               0.0|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.160328\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'RainTomorrow_index', maxIter=10, seed=1234)\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "gbt_predictions.select(\"prediction\", \"RainTomorrow_index\").show(5)\n",
    "\n",
    "# testing gradient boosted trees accuracy\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - gbt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you have to draw the graph (e.g. bar chart) to demonstrate the comparison of their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# preparing data for the plot\n",
    "accuracy_compare = [lr_accuracy, dt_accuracy, rf_accuracy, gbt_accuracy]\n",
    "legend = ['LR', 'DT', 'RF', 'GBT']\n",
    "\n",
    "# plotting\n",
    "bars = plt.bar(legend, accuracy_compare, align='center', color='lightblue', edgecolor='black')\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy by Model', fontsize=16)\n",
    "plt.xticks(size=14)\n",
    "plt.ylim(0.81, 0.84)\n",
    "# text on the top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(x = bar.get_x() + 0.1, y = yval - 0.003, s = str(round(yval*100,2))+'%', size =14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Answer:</font></b> <font color=blue><b>\n",
    "The worst model in terms of accuracy was logistic regression, followed by random forest, decision tree and gradient boosted trees, which was the best one. All the accuracies were found within a range of 81%-84%. The tree-based models presented a very similar accuracy metric. \n",
    "</font></b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 09: Calculate the confusion matrix and find the precision, recall, and F1 score of each classification algorithm. Explain how the accuracy of the predication can be improved?\n",
    "Finding the accuracy of the model does not always represent the quality of the model for a given dataset. Number of false positive and false negative identification also plays an important role when we decide about any particular classification model. The way we can calculate is called confusion matrix. You can use confusionMatrix() method to calculate the confusion matrix. From the confusion matrix show the precision, recall and f1 score of each classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR :\n",
      "\n",
      "Precision:  0.6339212131201583 \n",
      "Recall:  0.4038219235615288 \n",
      "F1 Score:  0.493361554743121 \n",
      "\n",
      "DT :\n",
      "\n",
      "Precision:  0.7310007524454477 \n",
      "Recall:  0.40802183956320875 \n",
      "F1 Score:  0.5237196765498652 \n",
      "\n",
      "RF :\n",
      "\n",
      "Precision:  0.7681975736568457 \n",
      "Recall:  0.372322553548929 \n",
      "F1 Score:  0.5015558698727015 \n",
      "\n",
      "GBT :\n",
      "\n",
      "Precision:  0.73477578084104 \n",
      "Recall:  0.44214615707685845 \n",
      "F1 Score:  0.5520812848246476 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# defining a function to take as argument the spark dataframe that contains the true values and predictions and \n",
    "# calculate the confusion matrix based on that:\n",
    "def confusion_matrix(spark_df):\n",
    "    # the MulticlassMetrics() functions takes as input an RDD of (prediction, label) pairs.\n",
    "    # We need to transform our dataframes of predictions to this shape:\n",
    "    rdd = spark_df.select(['prediction', 'RainTomorrow_index']).rdd.map(tuple)\n",
    "    # Using confusion matrix method from multiclass to return ou confusion matrix (as array) \n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    return metrics.confusionMatrix().toArray()\n",
    "\n",
    "# From the confusion matrix show the precision, recall and f1 score of each classification model:\n",
    "# Precision is the number of true positives divided by the number of true positives plus the number of false positives\n",
    "# recall is the number of true positives divided by the number of true positives plus the number of false negatives\n",
    "# F1 score is the harmonic mean of precision and recall \n",
    "models = ['LR', 'DT', 'RF', 'GBT']\n",
    "i=0\n",
    "for df in [lr_predictions, dt_predictions, rf_predictions, gbt_predictions]:\n",
    "    print(models[i],':\\n')\n",
    "    cm = confusion_matrix(df)\n",
    "    precision = cm[1,1]/(cm[0,1]+cm[1,1])\n",
    "    recall = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print('Precision: ', precision, '\\nRecall: ', recall, '\\nF1 Score: ', f1, '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how you can improve the accuracy of the prediction. <br>\n",
    "<font color=red><b>Answer:</font></b> <font color=blue><b>\n",
    "We could improve the Accuracy of them by introducing Cloud9am, Cloud3pm and Sunshine as features. If we are predicting rain tomorrow, it is logical that the fraction of sky obscured by cloud today and the number of hours of bright sunshine are correlated with the rain occurence in the next day, and helpful for our model. Other variables that were considered should be also analysed, maximum and minimum temperature, for instace, might not be so important for our model and maybe could  be eliminated from the analysis.\n",
    "\n",
    "Still on the feature engineering side, we can rescale the variables so that  the variables with higher scales such as pressure do not bias the model. We could also improve our analysis by using cross-validation to test the model's ability to predict new data, in order to flag problems like overfitting or selection bias.\n",
    "\n",
    "Furthermore, I can also improve the models by searching for the best hyper-parameters. For instance, for the 3 tree-based models, we could play with the parameter 'maxDepth',  having in mind that deeper trees are more expressive (potentially allowing higher accuracy), but they are also more costly to train and are more likely to overfit. For the ensemble models (GBT and Random Forest), the parameter 'numTrees'/'numInteractions' can be changed. By increasing the number of trees, the variance in predictions will decrease, improving the model’s test-time accuracy. It is important to mention that training time increases almost linearly as the number of trees increases. For the Logistic Regression, we can increase the number of interations with the parameter 'maxIter' to make the model understand the data patterns better, and so, improve its accuracy. (Apache Spark Documentation, 2019).\n",
    " \n",
    "</font></b> \n",
    "\n",
    "## References:\n",
    "\n",
    "Apache Spark Documentation, 2019, https://spark.apache.org/docs/2.3.0/ml-classification-regression.html <br>\n",
    "Apache Spark Documentation, 2019, https://spark.apache.org/docs/2.3.0/mllib-ensembles.html#gradient-boosted-trees-gbts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
